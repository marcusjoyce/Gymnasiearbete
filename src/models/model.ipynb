{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the data and split into training and testing.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "dirname = os.path.abspath('')\n",
    "\n",
    "X = np.load(os.path.join(dirname, '..', '..', 'data', 'X.npy'))\n",
    "y = np.load(os.path.join(dirname, '..', '..', 'data', 'y.npy'))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=576)\n",
    "\n",
    "del X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "993/993 [==============================] - 91s 90ms/step - loss: 0.5757 - accuracy: 0.7543 - val_loss: 0.4942 - val_accuracy: 0.7673\n",
      "Epoch 2/8\n",
      "993/993 [==============================] - 94s 95ms/step - loss: 0.5036 - accuracy: 0.7648 - val_loss: 0.4838 - val_accuracy: 0.7704\n",
      "Epoch 3/8\n",
      "993/993 [==============================] - 145s 146ms/step - loss: 0.4997 - accuracy: 0.7685 - val_loss: 0.4804 - val_accuracy: 0.7796\n",
      "Epoch 4/8\n",
      "993/993 [==============================] - 122s 123ms/step - loss: 0.4972 - accuracy: 0.7695 - val_loss: 0.4831 - val_accuracy: 0.7725\n",
      "Epoch 5/8\n",
      "993/993 [==============================] - 108s 108ms/step - loss: 0.4937 - accuracy: 0.7716 - val_loss: 0.4859 - val_accuracy: 0.7732\n",
      "Epoch 6/8\n",
      "993/993 [==============================] - 98s 98ms/step - loss: 0.4951 - accuracy: 0.7731 - val_loss: 0.4879 - val_accuracy: 0.7780\n",
      "Epoch 7/8\n",
      "993/993 [==============================] - 101s 102ms/step - loss: 0.4930 - accuracy: 0.7743 - val_loss: 0.4951 - val_accuracy: 0.7671\n",
      "Epoch 8/8\n",
      "993/993 [==============================] - 112s 113ms/step - loss: 0.4913 - accuracy: 0.7744 - val_loss: 0.4891 - val_accuracy: 0.7698\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train model\n",
    "\"\"\"\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, Dense, Dropout, Activation\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "\n",
    "batch_size = 64\n",
    "maxlen = 300\n",
    "embedding_dims = 100 #Length of the token vectors\n",
    "filters = 128 #number of filters in your Convnet\n",
    "kernel_size = 5 # a window size of 5 tokens\n",
    "epochs = 8\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1, input_shape=(maxlen,embedding_dims), kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(256, activation='leaky_relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#model.compile(loss = 'mean_squared_error',optimizer = 'adam', metrics=[soft_acc])\n",
    "model.compile(loss = 'binary_crossentropy',optimizer = 'adam', metrics=['accuracy'])\n",
    "model.fit(X_train,y_train,batch_size = batch_size,epochs = epochs , validation_data = (X_test,y_test))\n",
    "del X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "stop_words = set(map(lambda x : re.sub(r'[^\\w\\s]+', '', x), stopwords.words('english')))  # Loads nltk stopwords and removes punctuation\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "\n",
    "dirname = os.path.abspath('')\n",
    "filename = os.path.join(dirname, '..', '..', 'data', 'word2vec.100d.txt')\n",
    "\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "\n",
    "def vectorize(sentence):\n",
    "    \"\"\"\n",
    "    Takes a list of words and returns a list of vectors through word embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    out = np.empty((maxlen, embedding_dims))\n",
    "    i_ = 0\n",
    "    for i, word in enumerate(sentence):\n",
    "        if i < maxlen:\n",
    "            try:\n",
    "                out[i] = word2vec_model[word]\n",
    "            except KeyError:\n",
    "                out[i] = np.zeros(embedding_dims)\n",
    "            i_ += 1\n",
    "    out[range(i_+1, maxlen)] = np.zeros(embedding_dims)  # pad the array with arrays of zeros.\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def convert(sentence):\n",
    "    sentence = re.sub(r'[^\\w\\s]+', '', sentence).lower()\n",
    "\n",
    "    sentence = re.split(r'\\s+', sentence)\n",
    "\n",
    "    sentence = [word for word in sentence if word not in stop_words]\n",
    "\n",
    "    sentence = [wnl.lemmatize(wnl.lemmatize(word), pos='v') for word in sentence]\n",
    "\n",
    "    sentence = vectorize(sentence)\n",
    "\n",
    "    sentence = np.reshape(sentence, (1, maxlen, embedding_dims))\n",
    "    return sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "[[0.7911954]]\n"
     ]
    }
   ],
   "source": [
    "a = model.predict(convert(\"I fucking hate you, go to hell!\"))\n",
    "\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('gymnasiearbete_venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f8f9a1c5dea66502c36bf9832faacb0aadd87eec192002b66fedfa188ddbb25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
